name: Publish ADF Datasets

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment'
        type: string
        required: true
      function_name:
        description: 'Data source'
        type: string
        required: true
      date:
        description: 'Commit date'
        type: string
        required: true

env:
  ENVIRONMENT: ${{ inputs.environment }}
  DATE: ${{ inputs.date }}
  FUNCTION_NAME: ${{ inputs.function_name }}
  BRANCH: ${{ github.ref_name }}

jobs:
  publish:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2
        with:
          ref: ${{ github.ref_name }}

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      - name: Install Azure CLI datafactory extension
        run: |
          az extension add --name datafactory

      - name: Get changed files since provided date
        id: changed-files-since
        uses: tj-actions/changed-files@v37
        with:
          since: "${{ env.DATE }}"
          files: "dataproducts/${{ env.FUNCTION_NAME }}/datasets/"

      - name: Validate and Debug JSON Files
        run: |
          CHANGED_FILES="${{ steps.changed-files-since.outputs.all_changed_files }}"
          echo "Changed files: $CHANGED_FILES"

          if [[ -n "$CHANGED_FILES" ]]; then
            for file in $CHANGED_FILES; do
              echo "Processing file: $file"
              if [[ -f "$file" && "$file" == *.json ]]; then
                echo "File exists: $file"
                cat "$file"

                # Validate JSON structure
                if jq empty "$file"; then
                  echo "Valid JSON: $file"
                else
                  echo "Invalid JSON structure in: $file"
                  exit 1
                fi
              else
                echo "File not found or invalid: $file"
                exit 1
              fi
            done
          else
            echo "No changed files detected."
            exit 0
          fi

      - name: Publish ADF Datasets
        run: |
          if [[ "$ENVIRONMENT" == "dev" ]]; then
            az login --service-principal --username "${{ secrets.CLIENT }}" --password "${{ secrets.CLIENTSECRET }}" --tenant "${{ secrets.TENANT }}"
            az account set -s "${{ secrets.SUBSCRIPTION_ID }}"
            FACTORY_NAME="adf-psasdi-westus-dev-01"
            RESOURCE_GROUP="rg-psas-decision-intelligence-westus-dev"
          elif [[ "$ENVIRONMENT" == "qa" ]]; then
            az login --service-principal --username "${{ secrets.CLIENT_ID_QAT }}" --password "${{ secrets.CLIENT_SECRET_QAT }}" --tenant "${{ secrets.TENANT_ID_QAT }}"
            az account set -s "${{ secrets.SUBSCRIPTION_ID_QAT }}"
            FACTORY_NAME="${{ secrets.FACTORY_NAME_QAT }}"
            RESOURCE_GROUP="${{ secrets.RESOURCE_GROUP_QAT }}"
          elif [[ "$ENVIRONMENT" == "prod" ]]; then
            az login --service-principal --username "${{ secrets.CLIENT_ID_PROD }}" --password "${{ secrets.CLIENT_SECRET_PROD }}" --tenant "${{ secrets.TENANT_ID_PROD }}"
            az account set -s "${{ secrets.SUBSCRIPTION_ID_PROD }}"
            FACTORY_NAME="${{ secrets.FACTORY_NAME_PROD }}"
            RESOURCE_GROUP="${{ secrets.RESOURCE_GROUP_PROD }}"
          fi

          az account show
          echo "Logged in successfully"

          CHANGED_FILES="${{ steps.changed-files-since.outputs.all_changed_files }}"
          echo "Changed files to deploy: $CHANGED_FILES"

          if [[ -n "$CHANGED_FILES" ]]; then
            for file in $CHANGED_FILES; do
              echo "Processing file: $file"
              if [[ -f "$file" && "$file" == *.json ]]; then
                dataset_name=$(basename "$file" .json)
                echo "Deploying dataset: $dataset_name"

                # Build JSON content dynamically using environment variables and inputs
                JSON_CONTENT=$(jq -n \
                  --arg name "$dataset_name" \
                  --arg description "for T_IMS_SEQ_CUST_MAP_CURR" \
                  --arg linkedServiceReference "LS_PSAS_DI_SQL_75" \
                  --arg type "SqlServerTable" \
                  --arg schema "dbo" \
                  --arg table "T_IMS_SEQ_CUST_MAP_CURR" \
                  '{
                    name: $name,
                    properties: {
                      description: $description,
                      linkedServiceName: {
                        referenceName: $linkedServiceReference,
                        type: "LinkedServiceReference"
                      },
                      annotations: [],
                      type: $type,
                      schema: [
                        {"name": "IMS_SEQ_ID", "type": "varchar"},
                        {"name": "MCK_CUST_NUM", "type": "varchar"},
                        {"name": "QI_OUTLET_NM", "type": "varchar"}
                        # Add all other schema fields here
                      ],
                      typeProperties: {
                        schema: $schema,
                        table: $table
                      }
                    },
                    type: "Microsoft.DataFactory/factories/datasets"
                  }')

                # Print the JSON content to verify
                echo "Generated JSON content:"
                echo "$JSON_CONTENT"

                # Deploy dataset with the generated JSON content by piping it into the az command
                echo "$JSON_CONTENT" | az datafactory dataset create \
                  --factory-name "$FACTORY_NAME" \
                  --resource-group "$RESOURCE_GROUP" \
                  --name "$dataset_name" \
                  --properties @-

                echo "Successfully deployed dataset: $dataset_name"
              else
                echo "Skipping invalid file: $file"
              fi
            done
          else
            echo "No dataset changes detected."
          fi
