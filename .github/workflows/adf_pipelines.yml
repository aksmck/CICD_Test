name: Publish ADF Pipelines

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment'
        type: string
        required: true
      function_name:
        description: 'Data source'
        type: string
        required: true
      adf_deployment_step:
        description: 'Pipeline steps'
        type: string
        required: true
      date:
        description: 'Commit date'
        type: string
        required: true

env:
  ENVIRONMENT: ${{ inputs.environment }}  # Passed from master workflow
  DATE: ${{ inputs.date }}                # Passed from master workflow
  FUNCTION_NAME: ${{ inputs.function_name }}  # Passed from master workflow
  ADF_PIPELINES_DEPLOYMENT_STEPS: ${{ inputs.adf_deployment_step }}  # Passed from master workflow
  BRANCH: ${{ github.ref_name }}

jobs:
  publish:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2
        with:
          ref: ${{ github.ref_name }}

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      - name: Get changed files since provided date
        id: changed-files-since
        uses: tj-actions/changed-files@v37
        with:
          since: "${{ env.DATE }}"
          files: "dataproducts/${{ env.FUNCTION_NAME }}/pipelines/"

      - name: Publish ADF Pipelines
        run: |
          if [[ $ENVIRONMENT == 'dev' ]]; then
            az login --service-principal --username "${{ secrets.CLIENT }}" --password "${{ secrets.CLIENTSECRET }}" --tenant "${{ secrets.TENANT }}"
            az account set -s ${{ secrets.SUBSCRIPTION_ID }}
            FACTORY_NAME="adf-psasdi-westus-dev-01"
            RESOURCE_GROUP="rg-psas-decision-intelligence-westus-dev"
          elif [[ $ENVIRONMENT == 'qa' ]]; then
            az login --service-principal --username "${{ secrets.CLIENT_ID_QAT }}" --password "${{ secrets.CLIENT_SECRET_QAT }}" --tenant "${{ secrets.TENANT_ID_QAT }}"
            az account set -s ${{ secrets.SUBSCRIPTION_ID_QAT }}
            FACTORY_NAME="${{ secrets.FACTORY_NAME_QAT }}"
            RESOURCE_GROUP="${{ secrets.RESOURCE_GROUP_QAT }}"
          elif [[ $ENVIRONMENT == 'prod' ]]; then
            az login --service-principal --username "${{ secrets.CLIENT_ID_PROD }}" --password "${{ secrets.CLIENT_SECRET_PROD }}" --tenant "${{ secrets.TENANT_ID_PROD }}"
            az account set -s ${{ secrets.SUBSCRIPTION_ID_PROD }}
            FACTORY_NAME="${{ secrets.FACTORY_NAME_PROD }}"
            RESOURCE_GROUP="${{ secrets.RESOURCE_GROUP_PROD }}"
          fi

          echo "Logged in successfully"

          # Check if 'all' is selected and deploy pipelines accordingly
          if [[ $ADF_PIPELINES_DEPLOYMENT_STEPS == "all" ]]; then
            for file in ${{ steps.changed-files-since.outputs.all_changed_files }}; do
              if [[ $file == dataproducts/${{ env.FUNCTION_NAME }}/pipelines/* && $file == *".json" ]]; then
                echo "Deploying pipeline with change after the date: $file"
                pipeline_name=${file##*/} 
                pipeline_name=${pipeline_name%.*}  # Remove the file extension

                # Deploy the pipeline to a specific folder
                folder_name="test_folder"  # Specify your folder here
                az datafactory pipeline create --factory-name $FACTORY_NAME --pipeline $file --name $pipeline_name --resource-group $RESOURCE_GROUP --folder "$folder_name"
              fi
            done
          else
            folder="dataproducts/${{ env.FUNCTION_NAME }}/pipelines/${{ env.ADF_PIPELINES_DEPLOYMENT_STEPS }}/"
            for file in ${{ steps.changed-files-since.outputs.all_changed_files }}; do
              if [[ $file == $folder* && $file == *".json" ]]; then
                echo "Deploying pipeline from $ADF_PIPELINES_DEPLOYMENT_STEPS folder: $file"
                pipeline_name=${file##*/} 
                pipeline_name=${pipeline_name%.*}  # Remove the file extension

                # Deploy the pipeline to a specific folder
                folder_name="test_folder"  # Specify your folder here
                az datafactory pipeline create --factory-name $FACTORY_NAME --pipeline $file --name $pipeline_name --resource-group $RESOURCE_GROUP --folder "$folder_name"
              fi
            done
          fi
