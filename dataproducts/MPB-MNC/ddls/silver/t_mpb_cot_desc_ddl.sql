CREATE TABLE IF NOT EXISTS $$unity_catalog_name.$$schema_name.$$delta_table_name (
  CLIENT STRING COMMENT 'Client; SOURCE : MANDT',
  MPB_COT_CD STRING COMMENT 'Customer MPB Class of Trade Code; SOURCE : YYMPB_COT',
  MPB_COT_DESC STRING COMMENT 'Customer MPB Class of Trade Description; SOURCE : BEZEI',
  _rescued_data STRING,
  ADF_RUN_ID STRING COMMENT 'ID for specific pipeline run loaded from landing. This run_id gets generated when the records gets loaded from source to landing',
  ADF_JOB_ID STRING COMMENT 'ID of the trigger that invokes the pieline. This job_id gets generated when the records gets loaded from source to landing',
  RECORD_LOAD_TIME TIMESTAMP DEFAULT current_timestamp COMMENT 'This is the default generated column using current timestamp when the record is loaded in the table',
  INPUT_FILE_NAME STRING COMMENT 'File name which is getting populated from bronze layer',
  DATABRICKS_RUN_ID STRING COMMENT 'run id of the Databricks job run. This gets generated from the silver notebook run',
  DATABRICKS_JOB_ID STRING COMMENT 'job id of the Databricks job run. This gets generated from the silver notebook run',
  INSERT_TS TIMESTAMP COMMENT 'This is the default generated column using current timestamp when the record is loaded in the table',
  UPDATE_TS TIMESTAMP COMMENT 'This is the default generated column using current timestamp when the record is loaded in the table',
  INTEGRATION_KEY STRING COMMENT 'Hash key created using combination of all the columns except the housekeeping columns',
  DATE_PART DATE GENERATED ALWAYS AS (DATE(RECORD_LOAD_TIME)) COMMENT 'This the date part generated from Record load time column',
  HOUR_PART INT GENERATED ALWAYS AS (HOUR(RECORD_LOAD_TIME)) COMMENT 'This the hour  part generated from Record load time column'
) USING DELTA PARTITIONED BY (DATE_PART, HOUR_PART) LOCATION '$$delta_table_location' TBLPROPERTIES (
'delta.feature.allowColumnDefaults' = 'supported',
'delta.feature.appendOnly' = 'supported',
'delta.feature.invariants' = 'supported',
'delta.minReaderVersion' = '1',
'delta.minWriterVersion' = '7',
'delta.enableChangeDataFeed' = true,
'spark.sql.files.ignoreMissingFiles' = true,
'delta.autoOptimize.optimizeWrite' = true);